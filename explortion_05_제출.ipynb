{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1zfmOWQ7mx9ACXewKflaUo-O472xQv1dE",
      "authorship_tag": "ABX9TyPPpRitgfAiQNSxl4ebehjZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gong-aipel/AIFFEL_quest-cr/blob/main/explortion_05_%EC%A0%9C%EC%B6%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpjL35yJ4Av1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeJtEnEi4C6L",
        "outputId": "fe56a2a9-3fcf-47d1-da97-c991b60fb8b5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load data\n",
        "data_path = tf.keras.utils.get_file(\"ChatbotData.csv\",\n",
        "    origin=\"https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv\")\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess(sentence):\n",
        "    sentence = sentence.strip()\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r\"[^\\uAC00-\\uD7A3a-zA-Z?.!,]+\", \" \", sentence)\n",
        "    sentence = re.sub(r'\\s+', \" \", sentence)\n",
        "    return sentence\n",
        "\n",
        "questions = [preprocess(q) for q in data['Q']]\n",
        "answers = [preprocess(a) for a in data['A']]\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
        "START_TOKEN, END_TOKEN = [VOCAB_SIZE - 2], [VOCAB_SIZE - 1]\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "def encode(sentence):\n",
        "    return START_TOKEN + tokenizer.encode(sentence) + END_TOKEN\n",
        "\n",
        "input_tensor = [encode(q) for q in questions]\n",
        "output_tensor = [encode(a) for a in answers]\n",
        "\n",
        "input_tensor, output_tensor = zip(*[(inp, out) for inp, out in zip(input_tensor, output_tensor) if len(inp) <= MAX_LENGTH and len(out) <= MAX_LENGTH])\n",
        "\n",
        "input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, maxlen=MAX_LENGTH, padding='post')\n",
        "output_tensor = tf.keras.preprocessing.sequence.pad_sequences(output_tensor, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': input_tensor,\n",
        "        'dec_inputs': output_tensor[:, :-1]\n",
        "    },\n",
        "    output_tensor[:, 1:]\n",
        ")).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, position, d_model):\n",
        "        super().__init__()\n",
        "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "        self.pos_encoding = tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        return pos / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "\n",
        "    def call(self, x):\n",
        "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
        "\n",
        "# Multi-head Attention Encoder/Decoder Layers\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training=False, mask=None):\n",
        "        attn_output = self.mha(x, x, x, attention_mask=mask)\n",
        "        out1 = self.layernorm1(x + self.dropout1(attn_output, training=training))\n",
        "        ffn_output = self.ffn(out1)\n",
        "        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
        "        attn1 = self.mha1(x, x, x, attention_mask=look_ahead_mask)\n",
        "        out1 = self.layernorm1(x + self.dropout1(attn1, training=training))\n",
        "        attn2 = self.mha2(out1, enc_output, enc_output, attention_mask=padding_mask)\n",
        "        out2 = self.layernorm2(out1 + self.dropout2(attn2, training=training))\n",
        "        ffn_output = self.ffn(out2)\n",
        "        return self.layernorm3(out2 + self.dropout3(ffn_output, training=training))\n",
        "\n",
        "# Encoder/Decoder\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training=False, mask=None):\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(tf.shape(x)[-1], tf.float32))\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        for layer in self.enc_layers:\n",
        "            x = layer(x, training=training, mask=mask)\n",
        "        return x\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(tf.shape(x)[-1], tf.float32))\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        for layer in self.dec_layers:\n",
        "            x = layer(x, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)\n",
        "        return x\n",
        "\n",
        "# Masking 함수 정의 (누락되었던 부분)\n",
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = tf.cast(tf.math.not_equal(inp, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :]\n",
        "    dec_padding_mask = tf.cast(tf.math.not_equal(inp, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((tf.shape(tar)[1], tf.shape(tar)[1])), -1, 0)\n",
        "    dec_target_padding_mask = tf.cast(tf.math.not_equal(tar, 0), tf.float32)\n",
        "    combined_mask = tf.maximum(look_ahead_mask, 1 - dec_target_padding_mask[:, tf.newaxis, tf.newaxis, :])\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "# Transformer\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        inp, tar = inputs['inputs'], inputs['dec_inputs']\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar)\n",
        "        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)\n",
        "        dec_output = self.decoder(tar, enc_output, training=training, look_ahead_mask=combined_mask, padding_mask=dec_padding_mask)\n",
        "        return {'outputs': self.final_layer(dec_output)}\n",
        "\n",
        "# 하이퍼파라미터 및 모델\n",
        "model = Transformer(\n",
        "    num_layers=2,\n",
        "    d_model=256,\n",
        "    num_heads=8,\n",
        "    dff=512,\n",
        "    input_vocab_size=VOCAB_SIZE,\n",
        "    target_vocab_size=VOCAB_SIZE,\n",
        "    pe_input=MAX_LENGTH,\n",
        "    pe_target=MAX_LENGTH,\n",
        ")\n",
        "\n",
        "# 손실 및 학습 설정\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(y_true, y_pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
        "    loss = loss_object(y_true, y_pred)\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    return tf.reduce_sum(loss * mask) / tf.reduce_sum(mask)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=loss_function, metrics=['accuracy'])\n",
        "\n",
        "# 학습\n",
        "EPOCHS = 10\n",
        "model.fit(train_dataset, epochs=EPOCHS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nfLdDHp4OYN",
        "outputId": "74b013a2-9ecc-4690-f176-aeaa3db83794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv\n",
            "\u001b[1m889842/889842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2747s\u001b[0m 15s/step - accuracy: 0.0459 - loss: 7.5522\n",
            "Epoch 2/10\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2655s\u001b[0m 14s/step - accuracy: 0.0548 - loss: 5.5261\n",
            "Epoch 3/10\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2707s\u001b[0m 15s/step - accuracy: 0.0739 - loss: 4.5072\n",
            "Epoch 4/10\n",
            "\u001b[1m174/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2:41\u001b[0m 15s/step - accuracy: 0.0886 - loss: 3.7842"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(sentence):\n",
        "    # 전처리: 공백 제거 + 토크나이징\n",
        "    sentence = tf.expand_dims(START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "    # 디코더 입력은 시작 토큰으로 시작\n",
        "    output = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "        # 입력 마스크 생성\n",
        "        predictions = model(inputs={\n",
        "            'inputs': sentence,\n",
        "            'dec_inputs': output\n",
        "        }, training=False)\n",
        "\n",
        "        # 마지막 시퀀스 추출\n",
        "        predictions = predictions[:, -1:, :]\n",
        "        predicted_id = tf.argmax(predictions, axis=-1, output_type=tf.int32)\n",
        "\n",
        "        # 종료 토큰이면 멈춤\n",
        "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "            break\n",
        "\n",
        "        # 예측 결과를 디코더 입력에 추가\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    # 토크나이저로 디코딩\n",
        "    predicted_sentence = tokenizer.decode([i for i in tf.squeeze(output, axis=0) if i < tokenizer.vocab_size])\n",
        "\n",
        "    return predicted_sentence\n"
      ],
      "metadata": {
        "id": "vVaQRVjsb-dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict(\"안녕하세요\"))\n",
        "print(predict(\"오늘 기분 어때?\"))\n",
        "print(predict(\"너는 누구니?\"))\n"
      ],
      "metadata": {
        "id": "s5CK9DG4cAmC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}